{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33bxF3DpZ7yx",
        "outputId": "c2e43d79-d011-4ad7-e0ff-c68efdb6f1bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9esLtEZ6aRXZ"
      },
      "source": [
        "#Dwnlading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Y5UTfu8SaXaB",
        "outputId": "842276b9-bf4f-4036-f705-ebccad23e588"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3726308068.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 1. Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# This will prompt you to authorize access to your Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Define your desired folder in Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m     case = d.expect([\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect\u001b[0;34m(self, pattern, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mcompiled_pattern_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_pattern_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         return self.expect_list(compiled_pattern_list,\n\u001b[0m\u001b[1;32m    355\u001b[0m                 timeout, searchwindowsize, async_)\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mexpect_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     def expect_exact(self, pattern_list, timeout=-1, searchwindowsize=-1,\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pexpect/expect.py\u001b[0m in \u001b[0;36mexpect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mincoming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincoming\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Keep reading until exception or return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_base_path = \"/content/drive/MyDrive/Depression_Paper_DL\"\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(drive_base_path, exist_ok=True)\n",
        "\n",
        "def download_and_move_to_drive(dataset_handle, folder_name):\n",
        "    print(f\"Downloading {dataset_handle}...\")\n",
        "\n",
        "    # Download using kagglehub (saves to local runtime cache first)\n",
        "    cache_path = kagglehub.dataset_download(dataset_handle)\n",
        "\n",
        "    # Define the final destination for this specific dataset\n",
        "    destination_path = os.path.join(drive_base_path, folder_name)\n",
        "\n",
        "    # Move the files from cache to Google Drive\n",
        "    # dirs_exist_ok=True allows overwriting if you run this multiple times\n",
        "    shutil.copytree(cache_path, destination_path, dirs_exist_ok=True)\n",
        "\n",
        "    print(f\"\u2705 Dataset moved to: {destination_path}\")\n",
        "\n",
        "# --- Dataset 1: Suicide Watch ---\n",
        "download_and_move_to_drive(\"nikhileswarkomati/suicide-watch\", \"suicide_watch_data\")\n",
        "\n",
        "# --- Dataset 2: Sentiment140 ---\n",
        "download_and_move_to_drive(\"kazanova/sentiment140\", \"sentiment140_data\")\n",
        "\n",
        "print(\"\\nAll downloads complete. Check your Google Drive folder.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvPxfzKGbmG3"
      },
      "source": [
        "# Phase 1: Data Acquisition, Sampling & Relabeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmXrXXH-cGOH"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Define your file paths here (YOU MUST EDIT THESE PATHS)\n",
        "# Upload your raw CSV files to a folder in Drive (e.g., 'Colab Notebooks/Depression_Project')\n",
        "base_path = '/content/drive/MyDrive/Depression_Paper_DL'\n",
        "s140_path = os.path.join(base_path, 'training.1600000.processed.noemoticon.csv') # Std Sentiment140 filename\n",
        "sw_path = os.path.join(base_path, 'Suicide_Detection.csv') # Common Suicide-Watch filename\n",
        "\n",
        "# Output paths for the 30% sampled files\n",
        "s140_output_path = os.path.join(base_path, 'sentiment140_sampled_30.csv')\n",
        "sw_output_path = os.path.join(base_path, 'suicidewatch_sampled_30.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijBa1D5zd_Is",
        "outputId": "3c232676-9196-4641-a74f-1bfe9452fa5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Processing Sentiment140 Dataset ---\n",
            "Original S140 shape: (1600000, 6)\n",
            "Sampled S140 shape: (480000, 6)\n",
            " Saved sampled Sentiment140 to: /content/drive/MyDrive/Depression_Paper_DL/sentiment140_sampled_30.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ==========================================\n",
        "# 2. Process Sentiment140 Dataset\n",
        "# ==========================================\n",
        "print(\"\\n--- Processing Sentiment140 Dataset ---\")\n",
        "\n",
        "try:\n",
        "    # manually adding headers\n",
        "    cols = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
        "    df_s140 = pd.read_csv(s140_path, encoding='latin-1', names=cols)\n",
        "    print(f\"Original S140 shape: {df_s140.shape}\")\n",
        "\n",
        "    # Sampling (Reduce to 30%)\n",
        "    df_s140_sample = df_s140.sample(frac=0.3, random_state=42)\n",
        "    print(f\"Sampled S140 shape: {df_s140_sample.shape}\")\n",
        "\n",
        "    label_mapping = {0: 1, 4: 0}\n",
        "    df_s140_sample['target'] = df_s140_sample['target'].map(label_mapping)\n",
        "\n",
        "    df_s140_sample = df_s140_sample.dropna(subset=['target'])\n",
        "\n",
        "    # Save to Drive\n",
        "    df_s140_sample.to_csv(s140_output_path, index=False)\n",
        "    print(f\" Saved sampled Sentiment140 to: {s140_output_path}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\" Error: Sentiment140 file not found at {s140_path}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YNZdvrjeEYu",
        "outputId": "9294cec4-c906-401b-8d53-3faaacd2dc74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Processing Suicide-Watch Dataset ---\n",
            "Original Suicide-Watch shape: (232074, 3)\n",
            "Sampled Suicide-Watch shape: (69622, 3)\n",
            " Saved sampled Suicide-Watch to: /content/drive/MyDrive/Depression_Paper_DL/suicidewatch_sampled_30.csv\n",
            "\n",
            "Phase 1 Complete. Datasets are ready for Preprocessing (Phase 2).\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 3. Process Suicide-Watch Dataset\n",
        "# ==========================================\n",
        "print(\"\\n--- Processing Suicide-Watch Dataset ---\")\n",
        "\n",
        "try:\n",
        "    # Load Suicide-Watch dataset\n",
        "    df_sw = pd.read_csv(sw_path)\n",
        "    print(f\"Original Suicide-Watch shape: {df_sw.shape}\")\n",
        "\n",
        "    # Step: Sampling (Reduce to 30%)\n",
        "    df_sw_sample = df_sw.sample(frac=0.3, random_state=42)\n",
        "    print(f\"Sampled Suicide-Watch shape: {df_sw_sample.shape}\")\n",
        "\n",
        "    if 'class' in df_sw_sample.columns:\n",
        "        # Standardize labels to 0 and 1\n",
        "        def map_sw_labels(label):\n",
        "            label = str(label).lower()\n",
        "            if 'suicide' in label or 'depression' in label:\n",
        "                return 1 # Depressed/Suicidal\n",
        "            elif 'teenager' in label or 'non-suicide' in label:\n",
        "                return 0 # Non-Depressed\n",
        "            return None\n",
        "\n",
        "        df_sw_sample['target'] = df_sw_sample['class'].apply(map_sw_labels)\n",
        "\n",
        "        # Drop rows where label mapping failed\n",
        "        df_sw_sample = df_sw_sample.dropna(subset=['target'])\n",
        "\n",
        "        # Save to Drive\n",
        "        df_sw_sample.to_csv(sw_output_path, index=False)\n",
        "        print(f\" Saved sampled Suicide-Watch to: {sw_output_path}\")\n",
        "    else:\n",
        "        print(\" 'class' column not found. Check your CSV headers.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\" Error: Suicide-Watch file not found at {sw_path}\")\n",
        "\n",
        "print(\"\\nPhase 1 Complete. Datasets are ready for Preprocessing (Phase 2).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY2PjuODfjgD"
      },
      "source": [
        "#Phase 2: Data Preprocessing (Cleaning & Lemmatization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Yx1nqq9f0CP"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import pickle\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from google.colab import drive\n",
        "\n",
        "# Paths (Must match Phase 1 paths)\n",
        "base_path = '/content/drive/MyDrive/Depression_Paper_DL'\n",
        "s140_input_path = os.path.join(base_path, 'sentiment140_sampled_30.csv')\n",
        "sw_input_path = os.path.join(base_path, 'suicidewatch_sampled_30.csv')\n",
        "\n",
        "# Output paths\n",
        "s140_clean_path = os.path.join(base_path, 'sentiment140_cleaned.csv')\n",
        "sw_clean_path = os.path.join(base_path, 'suicidewatch_cleaned.csv')\n",
        "tfidf_path = os.path.join(base_path, 'tfidf_vectorizer.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pPc34vefjzf",
        "outputId": "0eabbb8f-6f25-4067-d1e4-d7f5379e9988"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading NLTK resources...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ==========================================\n",
        "# 1. Setup NLTK Resources\n",
        "# ==========================================\n",
        "print(\"Downloading NLTK resources...\")\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt') # Required if using nltk tokenizer, though we use split() for speed often\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Initialize Lemmatizer and Stopwords [cite: 233, 235]\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJG7kIUygBx3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ==========================================\n",
        "# 2. Define Preprocessing Function\n",
        "# ==========================================\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Implements the cleaning pipeline described in the paper:\n",
        "    1. Remove special characters/URLs/Emoticons [cite: 227-228]\n",
        "    2. Lowercase [cite: 229]\n",
        "    3. Remove Stop words [cite: 233]\n",
        "    4. Lemmatization [cite: 235]\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) # Remove URLs\n",
        "    text = re.sub(r'@\\w+', '', text) # Remove user mentions (e.g., @user)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # Remove special chars & numbers (keep only letters/spaces)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n",
        "    tokens = text.split()\n",
        "    filtered_tokens = [\n",
        "        lemmatizer.lemmatize(word)  # Lemmatize\n",
        "        for word in tokens\n",
        "        if word not in stop_words   # Remove Stop words\n",
        "    ]\n",
        "\n",
        "    return \" \".join(filtered_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9fkXPYqgQgU",
        "outputId": "6d51ee25-6096-41c5-aca6-7ade44e6f6be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Preprocessing Sentiment140 ---\n",
            "Cleaning text... (this may take a few minutes)\n",
            "\u2705 Saved clean Sentiment140 to: /content/drive/MyDrive/Depression_Paper_DL/sentiment140_cleaned.csv\n",
            "\n",
            "--- Preprocessing Suicide-Watch ---\n",
            "Cleaning text...\n",
            "\u2705 Saved clean Suicide-Watch to: /content/drive/MyDrive/Depression_Paper_DL/suicidewatch_cleaned.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ==========================================\n",
        "# 3. Apply to Datasets\n",
        "# ==========================================\n",
        "print(\"\\n--- Preprocessing Sentiment140 ---\")\n",
        "try:\n",
        "    df_s140 = pd.read_csv(s140_input_path)\n",
        "    # Apply preprocessing (This takes time on large datasets)\n",
        "    print(\"Cleaning text... (this may take a few minutes)\")\n",
        "    df_s140['clean_text'] = df_s140['text'].apply(preprocess_text)\n",
        "\n",
        "    # Drop rows that became empty after cleaning\n",
        "    df_s140 = df_s140[df_s140['clean_text'].str.strip().astype(bool)]\n",
        "\n",
        "    # Save cleaned text version (Needed for Transformers later)\n",
        "    df_s140.to_csv(s140_clean_path, index=False)\n",
        "    print(f\"\u2705 Saved clean Sentiment140 to: {s140_clean_path}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"\u274c Sentiment140 input file not found. Run Phase 1 first.\")\n",
        "\n",
        "print(\"\\n--- Preprocessing Suicide-Watch ---\")\n",
        "try:\n",
        "    df_sw = pd.read_csv(sw_input_path)\n",
        "    print(\"Cleaning text...\")\n",
        "    df_sw['clean_text'] = df_sw['text'].apply(preprocess_text)\n",
        "\n",
        "    # Drop rows that became empty after cleaning\n",
        "    df_sw = df_sw[df_sw['clean_text'].str.strip().astype(bool)]\n",
        "\n",
        "    # Save cleaned text version\n",
        "    df_sw.to_csv(sw_clean_path, index=False)\n",
        "    print(f\"\u2705 Saved clean Suicide-Watch to: {sw_clean_path}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"\u274c Suicide-Watch input file not found. Run Phase 1 first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDFXCXzGgZw_",
        "outputId": "1a234b61-4f3d-4f04-9602-08d1f4b2829c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Initializing & Fitting TF-IDF Vectorizer ---\n",
            "Fitting on 477587 tweets... this might take a moment.\n",
            " TF-IDF Vectorizer fitted and saved to: /content/drive/MyDrive/Depression_Paper_DL/tfidf_vectorizer.pkl\n",
            "   - Vocabulary size: 5000 words\n"
          ]
        }
      ],
      "source": [
        "# Feature enginering\n",
        "import pickle\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from google.colab import drive\n",
        "\n",
        "# We need the text to learn the vocabulary (the \"fit\" step)\n",
        "if 'df_s140' not in locals():\n",
        "    print(\"Loading cleaned Sentiment140 dataset...\")\n",
        "    try:\n",
        "        df_s140 = pd.read_csv(s140_clean_path)\n",
        "        # Ensure no NaNs exist after reloading\n",
        "        df_s140 = df_s140.dropna(subset=['clean_text'])\n",
        "    except FileNotFoundError:\n",
        "        print(f\"\u274c Error: File not found at {s140_clean_path}. Please run the Preprocessing step first.\")\n",
        "        df_s140 = None\n",
        "\n",
        "if df_s140 is not None:\n",
        "    # 3. Initialize TF-IDF Vectorizer\n",
        "    print(\"\\n--- Initializing & Fitting TF-IDF Vectorizer ---\")\n",
        "    # max_features=5000 is selected to keep the vector size manageable for Colab RAM\n",
        "    # while retaining the most frequent/important words.\n",
        "    tfidf = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "    # 4. Fit the Vectorizer\n",
        "    # This learns the vocabulary (IDF) from the dataset.\n",
        "    # We fit on Sentiment140 as it is the larger, more general dataset.\n",
        "    print(f\"Fitting on {len(df_s140)} tweets... this might take a moment.\")\n",
        "    tfidf.fit(df_s140['clean_text'])\n",
        "\n",
        "    # 5. Save the Vectorizer\n",
        "    # We pickle the object so we can reload it in Phase 3 without refitting.\n",
        "    with open(tfidf_path, 'wb') as f:\n",
        "        pickle.dump(tfidf, f)\n",
        "\n",
        "    print(f\" TF-IDF Vectorizer fitted and saved to: {tfidf_path}\")\n",
        "    print(f\"   - Vocabulary size: {len(tfidf.vocabulary_)} words\")\n",
        "\n",
        "else:\n",
        "    print(\" Skipping TF-IDF fitting due to missing data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUTa50D-hx8Z"
      },
      "source": [
        "#Phase 3: Training Traditional ML Models (10-Fold CV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxAimEA-iPB1",
        "outputId": "2e56eed2-2308-4435-a7ba-1f11d60fa4bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Setup Complete. Ready to train individual models.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Scikit-Learn Imports (CPU-based workflow)\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer # <--- THIS WAS MISSING\n",
        "\n",
        "# RAPIDS cuML Imports (GPU-based models)\n",
        "from cuml.linear_model import LogisticRegression\n",
        "from cuml.naive_bayes import BernoulliNB\n",
        "from cuml.ensemble import RandomForestClassifier\n",
        "\n",
        "# Paths\n",
        "base_path = '/content/drive/MyDrive/Depression_Paper_DL'\n",
        "s140_path = os.path.join(base_path, 'sentiment140_cleaned.csv')\n",
        "sw_path = os.path.join(base_path, 'suicidewatch_cleaned.csv')\n",
        "\n",
        "# Helper function to convert Sparse Matrix -> Dense Array\n",
        "# (Required for GPU Random Forest)\n",
        "def to_dense(x):\n",
        "    return x.toarray()\n",
        "\n",
        "# Shared Loading Function\n",
        "def load_data(path):\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"\u274c Error: File not found at {path}\")\n",
        "        return None, None\n",
        "    df = pd.read_csv(path).dropna(subset=['clean_text', 'target'])\n",
        "    X = df['clean_text']\n",
        "    y = df['target'].astype('float32') # cuML expects float32\n",
        "    return X, y\n",
        "\n",
        "print(\"\u2705 Setup Complete. Ready to train individual models.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4NzbYzyunAy"
      },
      "outputs": [],
      "source": [
        "def build_pipeline(use_sklearn=True):\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from cuml.linear_model import LogisticRegression as cuLogReg\n",
        "\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=50000,\n",
        "        ngram_range=(1,2),\n",
        "        stop_words='english'\n",
        "    )\n",
        "\n",
        "    if use_sklearn:\n",
        "        clf = LogisticRegression(\n",
        "            C=1.0,               # SAFE VALUE\n",
        "            max_iter=2000,\n",
        "            solver=\"lbfgs\",\n",
        "            n_jobs=-1\n",
        "        )\n",
        "    else:\n",
        "        clf = cuLogReg(\n",
        "            C=1.0,\n",
        "            max_iter=500,\n",
        "            tol=1e-4\n",
        "        )\n",
        "\n",
        "    from sklearn.pipeline import Pipeline\n",
        "    return Pipeline([(\"tfidf\", vectorizer), (\"clf\", clf)])\n",
        "\n",
        "def run_logistic_regression(dataset_path, dataset_name, force_sklearn=False):\n",
        "    print(f\"\\n\ud83d\udd39 Training Logistic Regression on {dataset_name}...\")\n",
        "\n",
        "    df = pd.read_csv(dataset_path)\n",
        "\n",
        "    # --- FIX: auto-detect label column ---\n",
        "    possible_label_cols = [\"label\", \"target\", \"sentiment\", \"class\", \"Label\"]\n",
        "    label_col = None\n",
        "\n",
        "    for col in possible_label_cols:\n",
        "        if col in df.columns:\n",
        "            label_col = col\n",
        "            break\n",
        "\n",
        "    if label_col is None:\n",
        "        raise ValueError(f\"\u274c No label column found in: {df.columns.tolist()}\")\n",
        "\n",
        "    # --- FIX: use correct text column name ---\n",
        "    possible_text_cols = [\"text\", \"tweet\", \"content\", \"message\", \"clean_text\"]\n",
        "    text_col = None\n",
        "\n",
        "    for col in possible_text_cols:\n",
        "        if col in df.columns:\n",
        "            text_col = col\n",
        "            break\n",
        "\n",
        "    if text_col is None:\n",
        "        raise ValueError(f\"\u274c No text column found in: {df.columns.tolist()}\")\n",
        "\n",
        "    X = df[text_col]\n",
        "    y = df[label_col]\n",
        "\n",
        "    # Clean Sentiment140 labels (0 and 4 \u2192 0 and 1)\n",
        "    # This part is likely redundant if loading 'cleaned.csv' files but kept for robustness\n",
        "    if dataset_name == \"Sentiment140\":\n",
        "        # Ensure 'target' column is present before trying to modify it\n",
        "        if 'target' in df.columns:\n",
        "             y = y.replace({4: 1})\n",
        "        else:\n",
        "             print(\"Warning: 'target' column not found for Sentiment140, skipping label remapping.\")\n",
        "\n",
        "    # Convert y to float32 early for cuML compatibility if used, and for consistency\n",
        "    y = y.astype('float32')\n",
        "\n",
        "    # --- NEW: Check for single class before cross-validation ---\n",
        "    if len(np.unique(y)) < 2:\n",
        "        print(f\"\u274c Error: Only one class found in {dataset_name} for target column '{label_col}'. Cannot perform binary classification.\")\n",
        "        print(f\"   Unique classes found: {np.unique(y)}\")\n",
        "        return # Exit the function if only one class is found\n",
        "\n",
        "    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "    scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
        "\n",
        "    # Use sklearn for Suicide Watch (or if force_sklearn is True)\n",
        "    use_sklearn = force_sklearn or (dataset_name == \"Suicide-Watch\")\n",
        "\n",
        "    pipeline = build_pipeline(use_sklearn=use_sklearn)\n",
        "\n",
        "    results = cross_validate(\n",
        "        pipeline, X, y,\n",
        "        cv=kfold, scoring=scoring,\n",
        "        error_score=\"raise\"\n",
        "    )\n",
        "\n",
        "    print(f\"   Accuracy:  {results['test_accuracy'].mean():.4f}\")\n",
        "    print(f\"   Precision: {results['test_precision'].mean():.4f}\")\n",
        "    print(f\"   Recall:    {results['test_recall'].mean():.4f}\")\n",
        "    print(f\"   F1-Score:  {results['test_f1'].mean():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fO3sc-PTyR_S",
        "outputId": "f40dd848-8734-46a4-960e-dfb27b7387ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83d\udd39 Training Logistic Regression on Sentiment140...\n",
            "   Accuracy:  0.7813\n",
            "   Precision: 0.7927\n",
            "   Recall:    0.7604\n",
            "   F1-Score:  0.7762\n"
          ]
        }
      ],
      "source": [
        "\n",
        "run_logistic_regression(s140_path, \"Sentiment140\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1qwYN79z5S7",
        "outputId": "83181232-e02e-49c4-9528-e9c65c2126ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83d\udd39 Training Logistic Regression on Suicide-Watch...\n",
            "\u274c Error: Only one class found in Suicide-Watch for target column 'target'. Cannot perform binary classification.\n",
            "   Unique classes found: [1.]\n"
          ]
        }
      ],
      "source": [
        "run_logistic_regression(sw_path, \"Suicide-Watch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B23Nm_xKuykm",
        "outputId": "5ddc5cb9-cc84-43f6-8680-d80aa5b61495"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83d\udd39 Training Bernoulli Naive Bayes on Sentiment140...\n",
            "   Accuracy:  0.7639\n",
            "   Precision: 0.7724\n",
            "   Recall:    0.7468\n",
            "   F1-Score:  0.7594\n",
            "\n",
            "\ud83d\udd39 Training Bernoulli Naive Bayes on Suicide-Watch...\n",
            "   Accuracy:  1.0000\n",
            "   Precision: 1.0000\n",
            "   Recall:    1.0000\n",
            "   F1-Score:  1.0000\n"
          ]
        }
      ],
      "source": [
        "# @title  Train Naive Bayes\n",
        "def run_naive_bayes(dataset_path, dataset_name):\n",
        "    print(f\"\\n\ud83d\udd39 Training Bernoulli Naive Bayes on {dataset_name}...\")\n",
        "\n",
        "    X, y = load_data(dataset_path)\n",
        "    if X is None: return\n",
        "\n",
        "    pipeline = make_pipeline(\n",
        "        TfidfVectorizer(max_features=5000),\n",
        "        BernoulliNB()\n",
        "    )\n",
        "\n",
        "    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "    scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
        "\n",
        "    try:\n",
        "        results = cross_validate(pipeline, X, y, cv=kfold, scoring=scoring)\n",
        "        print(f\"   Accuracy:  {results['test_accuracy'].mean():.4f}\")\n",
        "        print(f\"   Precision: {results['test_precision'].mean():.4f}\")\n",
        "        print(f\"   Recall:    {results['test_recall'].mean():.4f}\")\n",
        "        print(f\"   F1-Score:  {results['test_f1'].mean():.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u26a0\ufe0f Error: {e}\")\n",
        "\n",
        "# Run on both datasets\n",
        "run_naive_bayes(s140_path, \"Sentiment140\")\n",
        "run_naive_bayes(sw_path, \"Suicide-Watch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOG-VTKIvIWu",
        "outputId": "a24552fa-fc24-487f-a058-344f5f6173a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83d\udd39 Training Random Forest on Sentiment140...\n",
            "   (Note: Reducing features to 1000 and densifying data for GPU compatibility)\n",
            "   Accuracy:  0.6916\n",
            "   Precision: 0.7762\n",
            "   Recall:    0.5364\n",
            "   F1-Score:  0.6343\n",
            "\n",
            "\ud83d\udd39 Training Random Forest on Suicide-Watch...\n",
            "   (Note: Reducing features to 1000 and densifying data for GPU compatibility)\n",
            "   Accuracy:  1.0000\n",
            "   Precision: 1.0000\n",
            "   Recall:    1.0000\n",
            "   F1-Score:  1.0000\n"
          ]
        }
      ],
      "source": [
        "# @title Cell 4: Train Random Forest\n",
        "def run_random_forest(dataset_path, dataset_name):\n",
        "    print(f\"\\n\ud83d\udd39 Training Random Forest on {dataset_name}...\")\n",
        "    print(\"   (Note: Reducing features to 1000 and densifying data for GPU compatibility)\")\n",
        "\n",
        "    X, y = load_data(dataset_path)\n",
        "    if X is None: return\n",
        "\n",
        "    pipeline = make_pipeline(\n",
        "        TfidfVectorizer(max_features=1000),\n",
        "        FunctionTransformer(to_dense, accept_sparse=True),\n",
        "        RandomForestClassifier(n_estimators=100)\n",
        "    )\n",
        "\n",
        "    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "    scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
        "\n",
        "    try:\n",
        "        results = cross_validate(pipeline, X, y, cv=kfold, scoring=scoring)\n",
        "        print(f\"   Accuracy:  {results['test_accuracy'].mean():.4f}\")\n",
        "        print(f\"   Precision: {results['test_precision'].mean():.4f}\")\n",
        "        print(f\"   Recall:    {results['test_recall'].mean():.4f}\")\n",
        "        print(f\"   F1-Score:  {results['test_f1'].mean():.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u26a0\ufe0f Error: {e}\")\n",
        "\n",
        "# Run on both datasets\n",
        "run_random_forest(s140_path, \"Sentiment140\")\n",
        "run_random_forest(sw_path, \"Suicide-Watch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv09dQpD1AAp"
      },
      "source": [
        "#Phase 4: Training Transformer Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXvEcBGY1aoi",
        "outputId": "1974ce58-1bfe-452a-a28f-d0f78528324f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers accelerate evaluate datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9YODLfp1gQW",
        "outputId": "9dfe1050-4152-482e-b0bf-fb3abbcd220c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: WANDB_DISABLED=true\n"
          ]
        }
      ],
      "source": [
        "# Disable Weights & Biases logging\n",
        "%env WANDB_DISABLED=true\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "import evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ehh_nxekCGmD"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Paths\n",
        "base_path = '/content/drive/MyDrive/Depression_Paper_DL'\n",
        "s140_path = os.path.join(base_path, 'sentiment140_cleaned.csv')\n",
        "sw_path = os.path.join(base_path, 'suicidewatch_cleaned.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM7dFlSe1RXs"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 3. Training Function\n",
        "# ==========================================\n",
        "def train_transformer(dataset_path, dataset_name, model_checkpoint):\n",
        "    print(f\"\\n{'='*10} Fine-Tuning {model_checkpoint} on {dataset_name} {'='*10}\")\n",
        "\n",
        "    # 1. Load Data\n",
        "    if not os.path.exists(dataset_path):\n",
        "        print(f\"\u274c Error: File not found at {dataset_path}\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(dataset_path).dropna(subset=['clean_text', 'target'])\n",
        "\n",
        "    # METHODOLOGY CHECK: The paper limits Transformers to 50,000 entries per fold\n",
        "    # We sample 50k rows to match the paper's constraint and ensure Colab stability.\n",
        "    if len(df) > 50000:\n",
        "        print(f\"\u2139\ufe0f Sampling 50,000 rows (Paper Methodology Constraint)...\")\n",
        "        df = df.sample(n=50000, random_state=42)\n",
        "\n",
        "    # Ensure targets are integers for CrossEntropyLoss\n",
        "    df['label'] = df['target'].astype(int)\n",
        "    # Ensure text is strictly string format to avoid tokenization errors\n",
        "    df['clean_text'] = df['clean_text'].astype(str)\n",
        "    df = df[['clean_text', 'label']] # Keep only relevant columns\n",
        "\n",
        "    # [cite_start]2. Split Data (80% Train, 20% Eval) [cite: 290]\n",
        "    train_df, eval_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
        "\n",
        "    # Convert to Hugging Face Datasets\n",
        "    train_dataset = Dataset.from_pandas(train_df)\n",
        "    eval_dataset = Dataset.from_pandas(eval_df)\n",
        "\n",
        "    # 3. Tokenization\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        return tokenizer(examples[\"clean_text\"], truncation=True, padding=True, max_length=128)\n",
        "\n",
        "    print(\"\ud83d\udd39 Tokenizing data...\")\n",
        "    tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
        "    tokenized_eval = eval_dataset.map(preprocess_function, batched=True)\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    # 4. Initialize Model\n",
        "    # num_labels=2 because we have binary classes (Depressed vs Non-Depressed)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n",
        "\n",
        "    # [cite_start]5. Define Metrics (Accuracy, F1, Precision, Recall) [cite: 301]\n",
        "    accuracy_metric = evaluate.load(\"accuracy\")\n",
        "    precision_metric = evaluate.load(\"precision\")\n",
        "    recall_metric = evaluate.load(\"recall\")\n",
        "    f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "        acc = accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "        prec = precision_metric.compute(predictions=predictions, references=labels)\n",
        "        rec = recall_metric.compute(predictions=predictions, references=labels)\n",
        "        f1 = f1_metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "        return {\n",
        "            \"accuracy\": acc[\"accuracy\"],\n",
        "            \"precision\": prec[\"precision\"],\n",
        "            \"recall\": rec[\"recall\"],\n",
        "            \"f1\": f1[\"f1\"],\n",
        "        }\n",
        "\n",
        "    # 6. Training Arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results_{dataset_name}_{model_checkpoint.replace('/', '-')}\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=2,\n",
        "        weight_decay=0.01,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        push_to_hub=False,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "    )\n",
        "\n",
        "    # 7. Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train,\n",
        "        eval_dataset=tokenized_eval,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # 8. Train & Evaluate\n",
        "    print(\"\ud83d\udd39 Starting Training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"\ud83d\udd39 Final Evaluation:\")\n",
        "    metrics = trainer.evaluate()\n",
        "    print(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDZjjtnz4HGa"
      },
      "source": [
        "#training with different transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ybz4KW44R1D"
      },
      "source": [
        "distilbert-base-uncased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450,
          "referenced_widgets": [
            "d37bfad2a311432bb68d36fc416d96c2",
            "d1e694c0f8d04a428203a7f9acac3ec3",
            "e4b8b82274544dd89b1ce32f8c0fa2d6",
            "8e565ad1869242bb8dc34322661f1d2a",
            "b3f8da91451d4963b46316fc32809723",
            "7fdeca9cd4924daeb07e5eab3b42ddff",
            "7265b3477c004f50a7dadee7e497c428",
            "6395d09e2a4a42e09ab3f6164b587f0a",
            "86e1809afebc4d2ca344b2cdbde29b55",
            "9d48d09f19334e4bbe9bc3b67f9838a8",
            "56f8f7748f944fcda094df0c0c123b67",
            "3eb10e853fe84dac996d5b05ac65e6e8",
            "3204e7d791ad496aac00f422498cc758",
            "ee503086fc5d4e19a4417684f65f45ac",
            "89e9696780b14089be090f23facb615a",
            "8d9161b0ad39407eb5a1e086059f622d",
            "b53dcada36c8468b8c1207c27240df1f",
            "e4d17b3e8b214e6bae684ff9fbc9d3a4",
            "12c226fc6b344ad2bab5570453381829",
            "09dfe2f2541a4933a6f449aeef018b3b",
            "1c809ec2d57c481889e231ab276155b4",
            "30d96b5fa6e244588b523d401b418fc3"
          ]
        },
        "id": "jn4qSOeH4Bja",
        "outputId": "aa85bee3-8c28-410e-e9ca-c9ee930f58e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========== Fine-Tuning distilbert-base-uncased on Suicide-Watch ==========\n",
            "\u2139\ufe0f Sampling 50,000 rows (Paper Methodology Constraint)...\n",
            "\ud83d\udd39 Tokenizing data...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d37bfad2a311432bb68d36fc416d96c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3eb10e853fe84dac996d5b05ac65e6e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3829038943.py:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd39 Starting Training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5000/5000 05:59, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd39 Final Evaluation:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 00:11]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.4358033101634646e-07, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 11.3357, 'eval_samples_per_second': 882.168, 'eval_steps_per_second': 55.136, 'epoch': 2.0}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "MODEL_CHECKPOINT = \"distilbert-base-uncased\"\n",
        "\n",
        "# ==========================================\n",
        "# 4. Execution\n",
        "# ==========================================\n",
        "# Run DistilBERT on Suicide-Watch (Simple Patterns)\n",
        "train_transformer(sw_path, \"Suicide-Watch\", MODEL_CHECKPOINT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450,
          "referenced_widgets": [
            "5601ea1834a54440ae2b3ef9e0d72ed9",
            "d582923c9a24444ebfb7081854baee8f",
            "46c26d68d23f496baf615e5cc36cc08e",
            "c7ac7aa22a0543c9b8ed9bf187156a6b",
            "00410ba891b84daf82bbde941911ef9e",
            "f9f7b33e1615422d84659a9df748e172",
            "c9233061d3ca4fffa2c74282b95eef99",
            "d0d79229c8124d6389be0efb0c51a024",
            "886a860d2bff432f9f75e0cc503462e3",
            "2c2d7b287a26461a85d76ffc70e9d633",
            "73cbb068017542ef898385746a874d03",
            "7075b5724da84739bcb373fd0c05044b",
            "c97cbaf6d49b479da4bc509b744ae4da",
            "697a2e23bbdf487895251f605af87f6e",
            "fff61e0f22e24faeb40a0417d5ec80ca",
            "17b3d185f9654bf5b95ae20c31200785",
            "c3de526e60a442878a939c75774744d3",
            "39ba5c50e03f46d6b7037d854ebebfab",
            "53e672d0459547988fec7bdf83a45a80",
            "b3ac90a2b0fc43d3ac5758a5cc18dd8f",
            "3f0dda744e4c43c0ac72d59a01f2e9f0",
            "aec39565c9174df18c3e8358e3bc4306"
          ]
        },
        "id": "sexaS8x17LhL",
        "outputId": "3e46d4c1-1717-4e9f-f006-2549ea97282c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========== Fine-Tuning distilbert-base-uncased on Sentiment140 ==========\n",
            "\u2139\ufe0f Sampling 50,000 rows (Paper Methodology Constraint)...\n",
            "\ud83d\udd39 Tokenizing data...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5601ea1834a54440ae2b3ef9e0d72ed9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7075b5724da84739bcb373fd0c05044b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3829038943.py:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd39 Starting Training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5000/5000 06:01, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.472500</td>\n",
              "      <td>0.489963</td>\n",
              "      <td>0.774600</td>\n",
              "      <td>0.812758</td>\n",
              "      <td>0.711245</td>\n",
              "      <td>0.758621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.393500</td>\n",
              "      <td>0.476619</td>\n",
              "      <td>0.784100</td>\n",
              "      <td>0.782609</td>\n",
              "      <td>0.784337</td>\n",
              "      <td>0.783472</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd39 Final Evaluation:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 00:06]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.47661879658699036, 'eval_accuracy': 0.7841, 'eval_precision': 0.782608695652174, 'eval_recall': 0.7843373493975904, 'eval_f1': 0.7834720690001002, 'eval_runtime': 6.8199, 'eval_samples_per_second': 1466.296, 'eval_steps_per_second': 91.644, 'epoch': 2.0}\n"
          ]
        }
      ],
      "source": [
        "train_transformer(s140_path, \"Sentiment140\", MODEL_CHECKPOINT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a42NiYun4kpa"
      },
      "source": [
        "roberta-base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642,
          "referenced_widgets": [
            "920a75c677e849bca742eab110f45d69",
            "1214c53f7f9b4462a636037f9d219c24",
            "3475e5f21df140c596db9aed7513048f",
            "fb547a90b3424ac4aa3ad7e8a4179b82",
            "db8abc8308794a61b39ee213f478b922",
            "bb1ab0ae47e14cecb38e70dbbc33e422",
            "fe92358d11cf42ed80f4c1b6009838c0",
            "ef8ce9f0619d413781034c5c49e6eb91",
            "5eff28a5708d4be4a1e31e4b8e3078aa",
            "50146ad16fce4f3a89d5ffb9b60163af",
            "28742110640f46e383197214d2fdc2df",
            "a7d0f4557b5941f7b103524b3321819c",
            "88d3a64d06c44fc99b540debd343f0e5",
            "0d2fd4e82cac4356b0b1160d5de54408",
            "8a63677310e34bfeb2f90e3009c0df5c",
            "a7e074204f77431296956dd0336cc7fe",
            "e838e92df72448e88bcb96d523d45649",
            "3dc5b81948324050a5f7f614f7f66ee2",
            "798508b8ae58488aa145bd58d71415e7",
            "bc129dd31b684668b582598c155a9f67",
            "871cc9e08be74c609c92ec758b23b476",
            "25738a94c3394fafaec6ae5df1c0d446",
            "f46aea50ad114f3d8a900a6d05866d62",
            "1bf4be261e184a65bbe5bc2fb5ae3a9f",
            "6a49319b72e648e996ed145d046fe77e",
            "60aab768440e4b1eb426ec98a123a456",
            "9ca596a40e9f4bb98d1d3e64e142e8ca",
            "2101d016fd104cb58948089b3dfa4338",
            "a081722677ab441c9245a1fe5c561cec",
            "86a5de2f8377408eb34d3d6da889cf36",
            "471ee40e6a2e46d4a95d4a2c3b35373a",
            "30f840fd9e874b0b9eb6bdc684170577",
            "138138158b3e44f6992e71b43879f9ea",
            "fb1d278ad65040149c13bf896d4b8779",
            "861431aa8ee2491f8ab6a8e9a38501e4",
            "464523d6136e46be822d9086a99db4e3",
            "5855f9f7ef07479cae604ee45def443e",
            "caf4d009b059497186408a466ef0664c",
            "0633ee61e67b407db0bdd947ce7dde7e",
            "ccaa4ce732344119b9b6751229715aec",
            "97d1c3f887c547949f15da771c8311e8",
            "1ed4ff9670d6420d9d1f5e5eec78fc24",
            "bc98a9b53fca47ad8eab4fc4ce987b2b",
            "631dcb60ddcc4f17967eb939dbc984cf",
            "21657e24801e43d88830587995bc7a6c",
            "7e612bd33f80408695afd61738be383d",
            "3d0253edd42f4204aeedd99faf79bfef",
            "11bbda2ec103461b9ec63b7816463053",
            "8a10bfc7d88e4e9fa157fc19535c3f52",
            "f91561cb1f054422947331538c09f658",
            "c96cc6ce69ea4a45b36aeae7afe4b9f3",
            "7063f5d3bc2f4bbcb85ffff63b05aca1",
            "9421aab177e14a97a17a40489efc9da1",
            "4896d38b91db4fdc86531524a0cebb69",
            "aa88e7f6f9bf497dbfee5e1d73f2c734",
            "08bfaa1fab0e49b7ba348960a5c35f74",
            "e4e458c99f2c4d92bc54b1aa9fae99dc",
            "7c03776e8b534e95b59a59b3c85a4379",
            "6cf7884603a44b859811541c8ab6f580",
            "7643802aef5a4aa2a4490ff77aae01c1",
            "0f879fb684944ada9d94298a557a31de",
            "485eee075242402795227ad0007a87c9",
            "56723aef936b49b9a1c06f550d5ad267",
            "c278e2db03a24f75be7e8b6d68934a8b",
            "8c3c71bb25654aecb74a8f38bd34163b",
            "e20cefe47c58414aa4bbd912a159e670",
            "07fdac15b1154637b3ae587451c2b10b",
            "ae54f2ce71ed4c12b03ef8a79511372e",
            "4d3950e7ecd24daca1c955fa9781b001",
            "8b72266c35214b9a9d4775d0bb2f4db9",
            "b26fcbad8d464704965fe33b1afd156c",
            "16034bc34e1344afbc7a41108246e627",
            "428a93d868c346649bdc0223fad7765e",
            "baa7c8980c8a413398815d65047cb377",
            "20aa21fe14c244e893b48cbaa0c9eb35",
            "8713eca9d5344f7eb7978c3dccd3657b",
            "a847034a5b9c49a3b5691bad6be48e78",
            "366607a8a1b643ba8ee88cdf66c1d37c",
            "7af5c31a69514d3aba3048eaa306db35",
            "e88fccbe87d1411eb1e490a3a4588f4f",
            "fdb9f42f04c04504bc3fa343f100e038",
            "78585d59f6164b3884ff441e9e3f264c",
            "8d0437795a66438e82e2e450ff4d8cca",
            "86784c490f7748029d29ffee04400b70",
            "d5b8853d65be4af6a48a1f2f40318311",
            "2950a7569d684d67a3214162d9cd7c6c",
            "612bf257ff0149a2b7a8da384485ba14",
            "96f1d56f3fd44b89b49d16d692ce9ba2"
          ]
        },
        "id": "VQuFPAF-4hqe",
        "outputId": "67796362-2333-49d2-e4bc-d079d59be921"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========== Fine-Tuning roberta-base on Suicide-Watch ==========\n",
            "\u2139\ufe0f Sampling 50,000 rows (Paper Methodology Constraint)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "920a75c677e849bca742eab110f45d69",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7d0f4557b5941f7b103524b3321819c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f46aea50ad114f3d8a900a6d05866d62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb1d278ad65040149c13bf896d4b8779",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21657e24801e43d88830587995bc7a6c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd39 Tokenizing data...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08bfaa1fab0e49b7ba348960a5c35f74",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07fdac15b1154637b3ae587451c2b10b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "366607a8a1b643ba8ee88cdf66c1d37c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3829038943.py:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd39 Starting Training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5000/5000 15:17, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd39 Final Evaluation:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 00:19]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.0733604085544357e-06, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 19.809, 'eval_samples_per_second': 504.821, 'eval_steps_per_second': 31.551, 'epoch': 2.0}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "MODEL_CHECKPOINT = \"roberta-base\"\n",
        "\n",
        "# ==========================================\n",
        "# 4. Execution\n",
        "# ==========================================\n",
        "# Run DistilBERT on Suicide-Watch (Simple Patterns)\n",
        "train_transformer(sw_path, \"Suicide-Watch\", MODEL_CHECKPOINT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450,
          "referenced_widgets": [
            "2c640d2f64954e36b21b3a39f31bacb9",
            "26710310a46849a8b552c9621fc8b992",
            "980fdd7ea1c146e28be86fc9fa5cf656",
            "f2ac92483d104138910a966ee29a2d20",
            "ba39d69238464ecd87c1c034a75b1cfa",
            "3361924786ad410483837b77447db0d8",
            "b54fc0df0c994393b16d78ef8b388a4a",
            "a2f51685d23b4f2e9ac56406c549a2c3",
            "ad9ecb0cb9b6489580723a0c5e17d831",
            "5ab51f8e7c8a4a56b5ef6bffc95a0c36",
            "1180ac13ccac4a48a06a9a964489cd06",
            "e2d90ba3868945baa11b148eea9bcd02",
            "2f5d8d410bc64a869815decc53ebe110",
            "b3892ad6d76b4b5fa4972624243fbbe9",
            "c0ebaefff2484d9482e724a582085783",
            "a6a1fcca671446a6bd0d921e83f97d79",
            "872b80075d664e4a9892fc24fc5c5fd7",
            "229759b78284428399b6034f8dbd97e2",
            "bd47748dcca94d399da8cb87e5cb0213",
            "0071193ee9374b4c8eee9628f0c4dea2",
            "76222570362944fd8403e8fec477dfaa",
            "404713e778ea4d7f8a4c8f7f460c4690"
          ]
        },
        "id": "0-ce4sg-7Qqm",
        "outputId": "f0f873e5-2906-457d-cc4c-9dbabf9451d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========== Fine-Tuning roberta-base on Sentiment140 ==========\n",
            "\u2139\ufe0f Sampling 50,000 rows (Paper Methodology Constraint)...\n",
            "\ud83d\udd39 Tokenizing data...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c640d2f64954e36b21b3a39f31bacb9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2d90ba3868945baa11b148eea9bcd02",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3829038943.py:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd39 Starting Training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5000/5000 11:47, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.469500</td>\n",
              "      <td>0.507239</td>\n",
              "      <td>0.779300</td>\n",
              "      <td>0.834661</td>\n",
              "      <td>0.694378</td>\n",
              "      <td>0.758084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.406400</td>\n",
              "      <td>0.465947</td>\n",
              "      <td>0.796900</td>\n",
              "      <td>0.792734</td>\n",
              "      <td>0.801807</td>\n",
              "      <td>0.797245</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd39 Final Evaluation:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 00:10]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.4659474194049835, 'eval_accuracy': 0.7969, 'eval_precision': 0.7927337701012508, 'eval_recall': 0.8018072289156627, 'eval_f1': 0.7972446840371369, 'eval_runtime': 11.0862, 'eval_samples_per_second': 902.026, 'eval_steps_per_second': 56.377, 'epoch': 2.0}\n"
          ]
        }
      ],
      "source": [
        "train_transformer(s140_path, \"Sentiment140\", MODEL_CHECKPOINT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nslXloVhZDN5",
        "outputId": "9154fa9c-bfb3-4c1f-cdf9-120791c2ae04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current GPU Memory Allocated: 0.60 GB\n",
            "Current GPU Memory Reserved:  0.66 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# 1. Delete variables that might be holding GPU memory\n",
        "# We wrap in try/except in case they are not defined yet\n",
        "try:\n",
        "    del model\n",
        "    del trainer\n",
        "    del optimizer\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "# 2. Force Python's Garbage Collector to release unreferenced memory\n",
        "gc.collect()\n",
        "\n",
        "# 3. Clear PyTorch's internal cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 4. Verify memory status\n",
        "print(f\"Current GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "print(f\"Current GPU Memory Reserved:  {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoVErrWy4uLx"
      },
      "source": [
        "squeezebert/squeezebert-uncased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610,
          "referenced_widgets": [
            "95afbe7bfe9e4c68a93a0643a8f7059c",
            "dad1ddd725a14cd49ad50d2930221c81",
            "63c67e5ae9f543d7bb60ddb46e6ce6e3",
            "0a448db40d874e379b3b066efadfb6d8",
            "21a28d234c024ef08a7c3e55f79636bd",
            "751826e480be40d7b1572757c81961cf",
            "0e6b4a1ce3e34e53b3cd9a362b1c0c28",
            "531c76e87e63402aa02879eaafd1119a",
            "1c219077d54f42998dea9b12ad01ea62",
            "b193b9451fc54b00a32f188ca081f283",
            "8f2de963ddba4c7c8bf94b05ef3f867f",
            "36add072f0764aff956ca820360e6647",
            "a3c2ddc886b54674aaf321da372de73d",
            "f1b4239c1de44fb39a4e5fee90d2db2b",
            "23711410d5a7489386c7c2622ae6582b",
            "0b778496ab3340b8b617cce985a7844c",
            "57b781a99e7a4418939a261b3a358563",
            "b70ba805750f4139bc91db8bd2c571f8",
            "de28f9860e1541ef9e1932b78b5cf940",
            "044345f24fcb4a25a2e418a7c23ca19f",
            "eaac291bd6224884b7f84eaa4094d1da",
            "8675ab9985a3474c89d78911d7499bfd",
            "ace325f0309349f480c907b40689f39a",
            "51e8f3dfad0c487fa92722c560eea1c3",
            "090e33dd040b4b59b4c9e3cf86542b61",
            "6168a1220a7741a2ab96b535234c8e0d",
            "5f31a6019d72450fbf13605f4a89d2f8",
            "f7195870d0ae4b7db61df6cf6a62a27e",
            "5948d203e9954e7cafc7ff5e6182d6f4",
            "733fe5d6e0e94110b401fb6942ab674a",
            "668fa2d72f49437d80d6d798bf3d2ad5",
            "8e959c7d9b8e4518b761c8e92450576d",
            "996741a25ff443ab9f2f704f5227c6eb",
            "43124dce85184872b72d397549e50e48",
            "c9151d133d05438a9d01a904971651d0",
            "c91578b550a5454ea043f0c2460e5201",
            "b4cd45e4467a44409138082b9f441d36",
            "49b3fcf572c14afea072599257029438",
            "b37bad6981cf4575935be880b002f62a",
            "c445e93265294c32a5eec3df9c7d726c",
            "2fb8eb81a7a843cbb48f53bb00c37516",
            "8242e7d94a444ab0b24daa3b0ff59998",
            "d28ff8f99bd643209a4feaad5ef70091",
            "588c6a3da96644e3ae7a9523d9e6d9ce",
            "5bff25a9ecec4ffda7d0546cfacc2ebe",
            "b9d8aff3a9d44e0686a49c00f86a5094",
            "b4b70930942c40e7ace0052cad0c3573",
            "7896026769a84352b630f9101b2dc03a",
            "90ec54ffd9b54727aaf95c6b55928ca4",
            "2f6c9d740deb4fa283417630b8948854",
            "7db8c5fc9ce5473196592b7cfab192e2",
            "bbaee53cb4c049a39b16b5dd8a269b92",
            "d737d95a2a274d2ba0fa81dbea6540d2",
            "1a4bc89df1e945fd87bab72163e60882",
            "5638f24681f64689a3bfa5adf40c3ea5",
            "ebe8a969b68c47d3bb8860b46e44ef83",
            "fd6bda1d4a604b5ba894437224eebdd9",
            "0fb4835bc5f04fcfb6485e8bcb3bab6c",
            "5e79c445890c4009b6b8a36afa7db579",
            "0edd991e2017419c8caaccb30e1a17c2",
            "fb0453217a35486bad6276acb82518f4",
            "cae6e15f76524e8dbe0fdc6c208bc793",
            "45cdbbca7f2c400c8a9eea4583bc1c45",
            "f188e9c3897843cd8f405214fd247362",
            "1bfaf810d6b54032b184f3c731b5ca03",
            "56423b9b5eb149e2a93d3e1c622a74b2",
            "c65d155cfab04de1a10895e8ace09f40",
            "1cdaaf7f75af408e9619a08d962ff224",
            "0399a2ad69c94b498cb170abdb16ef53",
            "b9039170038541e2abf3c4f7305e9a48",
            "5125ed0d6063403e9f5f8d34219b77cb",
            "e57916dbc1934e08a8966a6170dfcf70",
            "5c0229226ed849c8857736e8571487a9",
            "db5d6e0ff66f4dce9e26d8704594233f",
            "43db27f9bc85460bbafedd36a7cf367f",
            "abb9d6cebb934f24896970484f39c886",
            "cca1339961f148d1883f3700ade7abcf"
          ]
        },
        "id": "fIjVgLHh4z0W",
        "outputId": "222e6163-ec2b-4686-fd66-554795861863"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========== Fine-Tuning squeezebert/squeezebert-uncased on Suicide-Watch ==========\n",
            "\u2139\ufe0f Sampling 50,000 rows (Paper Methodology Constraint)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95afbe7bfe9e4c68a93a0643a8f7059c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/500 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36add072f0764aff956ca820360e6647",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ace325f0309349f480c907b40689f39a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd39 Tokenizing data...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43124dce85184872b72d397549e50e48",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5bff25a9ecec4ffda7d0546cfacc2ebe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebe8a969b68c47d3bb8860b46e44ef83",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/103M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of SqueezeBertForSequenceClassification were not initialized from the model checkpoint at squeezebert/squeezebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c65d155cfab04de1a10895e8ace09f40",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/103M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3829038943.py:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd39 Starting Training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5000/5000 58:57, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd39 Final Evaluation:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 04:53]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 8.17704221844906e-06, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 294.549, 'eval_samples_per_second': 33.95, 'eval_steps_per_second': 2.122, 'epoch': 2.0}\n"
          ]
        }
      ],
      "source": [
        "MODEL_CHECKPOINT = \"squeezebert/squeezebert-uncased\"\n",
        "\n",
        "# ==========================================\n",
        "# 4. Execution\n",
        "# ==========================================\n",
        "# Run DistilBERT on Suicide-Watch (Simple Patterns)\n",
        "train_transformer(sw_path, \"Suicide-Watch\", MODEL_CHECKPOINT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842,
          "referenced_widgets": [
            "c0b3f8fee7504e108c311d36e478221a",
            "a86f75f36df64898a9c4d145ea1823f4",
            "9c3d75cc3c81469a9be7a83a9d785605",
            "61821616e2d84e5f85ad68e6d8816db4",
            "5bda8aec06ed4daea9b3dde9067b46ce",
            "83d38b38ee3d4790b10d2f741486126d",
            "957b93b972b9473b925268df94d1caf5",
            "13b8d7157b1c4a1196752119897e83ae",
            "4ee5533d1e42493a890e021d19f2bd19",
            "d0a8669cd88846318bff16553037a9ad",
            "e9d5e5dbb6194e1fb27982df804b998a",
            "2884b87b79cb4abcbbb8f380a3d50cba",
            "0bb5a6c422b141f1831c210aede7c1ca",
            "29e2a1f71feb495ba4f96c1ae3c62d5a",
            "5935545951a34ca39519c3a50d277281",
            "2236185a614a46b9a3e24ccc0c030f80",
            "bf8914fd28b541e288415be2d263e670",
            "92440be3e0da48059a7b16ea7623f4da",
            "aaf7fe9bb35049a2966e546007e3a907",
            "53d8814fb0664f83beb45e5f1dd75e8a",
            "c87af68beba64aebac2f5a9eba213589",
            "89af343b945f4b0bb7a1d90a6568a47d",
            "6ea4a596404947029f258a591a241fd4",
            "d916ae06a1a840c2bc0d61b2a9f79b56",
            "cdd6b5cde4b243c096be62e9ababf122",
            "3adcf2bdf2314b5dbaa93d4123154c61",
            "dc32e6e1e1ee48af9bfe30190032082b",
            "b92dcbedcf6b495089cb93a0f04ba67b",
            "05beeea3f373466082db4ea3dc5237b3",
            "f6044f2bbb714d9b99023ca8d4497f4e",
            "ca228dbc155b43a6a6782705e868a627",
            "0965123976e14b929b9f9addbbb80ded",
            "a0cd88fdfb01445abdfa2e236a8fe4d3",
            "6c14ce91788848e0b14c80fc56bab679",
            "3bf7b0e27e524213885c39329696400b",
            "cee39aab57404ef6acfb5d6ce70f42cf",
            "a9b0871937ce4dfa9188e038012b2dfc",
            "1717ae1b3acf48339339615c3729872b",
            "65c8157452cc4623bc844fb9d9fb6044",
            "1d5cd991795e4c40b26a54ce4d257b90",
            "704744c554964831b4f932e8881d8c8f",
            "5b8ff150acd041d58ce906a988cde99e",
            "2e852a1771294bef97d51c584f31e722",
            "7b12a0974d7e4533988e2f27fd736352",
            "160b5c77423f40d19d03191cc198bbc9",
            "59c2e583a53744cf80f4a14c80bdc0da",
            "996c80db68844d4aa221ae69b6171e16",
            "2d1b130ab18e40b392b5b341c6cf1e82",
            "1cd4a75bbdf94b0fa56607fcef899292",
            "f49d7bf8cb67423b936a06e1eef9de77",
            "02347a92667949048dbbce0fa0b8475a",
            "2cc7fbdfef354b81a44c0b2d34496088",
            "6e6e3006ae974c4994fa14be208ff673",
            "737e956b75814b21acf930ceeee6b23e",
            "c4647562cdd94c3d999b92650eac9577",
            "1d5f11df620045918575f2c9ed2d0a1d",
            "29a1b44f663844b2bda9ce2f62ce9267",
            "df5b37291014436196e2385378f20035",
            "aa5a890f39184fb4a706cd3bd3c7e1e6",
            "12c7de5d29a44355a2da4107efb2a4d2",
            "03de98bf79b14c4080277cea8ec068b8",
            "dd208124398b4dc196855f59be66e89e",
            "043a81418e904be1b20917f7b60dd9d0",
            "dd17cb4200ab4ea4942a0d4e453df3e7",
            "e81ada4dcea840b1b1e7837b87c2c95f",
            "3a7f1c386c3346c19ed2929372df2adf",
            "28f44122c8264e4998ceff2e90663a4f",
            "049e345c39684617aaeddb76093e4146",
            "bbf6b3e11a3f4dbe9e504fa6e3cd2b08",
            "a92e903e524f41a1bc4826b8e5af3f69",
            "f8efbc6539f2445196fb3684b6d9fe00",
            "f45cedfe63cf40b3bfc04daae444acce",
            "872673316ca44a17bec0aea0beec7ff6",
            "bf522295a30a40dd843302568ac6e15a",
            "69b8acaa91a8408bb5c6b675e5fe64b2",
            "8fcd0e3472e64345b9432a2403d15cf9",
            "3f884b9aa8fd49e0b26511f25ef12ca8",
            "b2a340df1e0c4b92a6d7709c506bb5ba",
            "0e2b14caf031439b94e0f9ebcc5d1c12",
            "cb43a02aa5d445bc9f94cb85a751d291",
            "8dfb57476e12474396c03cc7f7b7d500",
            "0d57a46a200440d6bbd8329ebf9d05eb",
            "1014683337e54654a183aa8e937913a9",
            "719ed6b7ead045afa33bdd3c4e98aca4",
            "41d58ea08abc41de94795ca2dc6b0a9f",
            "9ad0e7172c7049ac8438aafee505e8b2",
            "9a9dc2caaa464f56ab9d00200d44c1d7",
            "57e2a5e0e0004891a442f2822e2e84d3",
            "e4fe8096c93242e6b73ded1e6f2b2cb5",
            "dcde811053df4554b9e05b05f0182355",
            "03be56fe619b4484a7f97969f19da543",
            "797124daa1c745fe80456ccb39878d5e",
            "001d22718e9f4b9a8ed564c2145ed28f",
            "042ecc775c20404498e6cc97547091cd",
            "3e49e49d65a34b2ea33ff9b8ad256454",
            "4b8e237c60e746d3b737b95d59377c4a",
            "cc292488a41241c09f9d0ade40c7f096",
            "1f0ac626e30243c7ba5688476e8a0839",
            "40b7092aa1e949c1b083774e7999eb70",
            "fdc664091ea24985b6fa4b4a84d2a666",
            "8ee8ea77d9594919abf6ae3ada9eec1f",
            "c6e4ae4b7b4a4f6bb1fc10cd437bf687",
            "c62cf43277484dadbe788cb0db2ee220",
            "09930ca6732f495887a3f569eb43385a",
            "c3cc7f439ea14f8f8f927938b168e048",
            "999ade74d94c40138d5596fdca82983c",
            "3c423f722dcb4da9b878874f98de1076",
            "fc4ba9d4f58941c0a7876cfd575c2d8d",
            "c6760aa9e31048858df620960a2ad227",
            "9a78593883334c27aab2cfc8d5840115",
            "506152052666478e8a3a4e2bc885b5da",
            "b7ff76fac79d4737918ffa9fea1851cb",
            "9b4980e421254b609706ea3d8c1ce370",
            "84e9f5d2fcdc4cc7826182cb02415dd2",
            "4967cba12ed348d6a03cde7fea2626cc",
            "a260797e427e42a78a1a07a214b7b017",
            "63bdf79b213244a487da2621f767ab49",
            "4e6cf74833124eab98ed84d1d96c9bc4",
            "0fceee3e83e541bebd872b973cb88990",
            "49d8023cc55b4495b8d391fe068eae38",
            "8ef7d9d6fdd64b5b95086b868d6253de"
          ]
        },
        "id": "IGQmjUC27Sc6",
        "outputId": "61364735-d0d6-420b-a006-7fb77ec75312"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========== Fine-Tuning squeezebert/squeezebert-uncased on Sentiment140 ==========\n",
            "\u2139\ufe0f Sampling 50,000 rows (Paper Methodology Constraint)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0b3f8fee7504e108c311d36e478221a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/500 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2884b87b79cb4abcbbb8f380a3d50cba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ea4a596404947029f258a591a241fd4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd39 Tokenizing data...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c14ce91788848e0b14c80fc56bab679",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "160b5c77423f40d19d03191cc198bbc9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d5f11df620045918575f2c9ed2d0a1d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/103M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28f44122c8264e4998ceff2e90663a4f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/103M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of SqueezeBertForSequenceClassification were not initialized from the model checkpoint at squeezebert/squeezebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2a340df1e0c4b92a6d7709c506bb5ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4fe8096c93242e6b73ded1e6f2b2cb5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fdc664091ea24985b6fa4b4a84d2a666",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "506152052666478e8a3a4e2bc885b5da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-40049949.py:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd39 Starting Training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5000/5000 09:37, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.494500</td>\n",
              "      <td>0.511261</td>\n",
              "      <td>0.753900</td>\n",
              "      <td>0.803567</td>\n",
              "      <td>0.669478</td>\n",
              "      <td>0.730420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.455700</td>\n",
              "      <td>0.487372</td>\n",
              "      <td>0.773100</td>\n",
              "      <td>0.764076</td>\n",
              "      <td>0.787550</td>\n",
              "      <td>0.775635</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd39 Final Evaluation:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 00:17]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.4873715937137604, 'eval_accuracy': 0.7731, 'eval_precision': 0.7640755893239821, 'eval_recall': 0.7875502008032128, 'eval_f1': 0.7756353208741225, 'eval_runtime': 18.0894, 'eval_samples_per_second': 552.81, 'eval_steps_per_second': 34.551, 'epoch': 2.0}\n"
          ]
        }
      ],
      "source": [
        "MODEL_CHECKPOINT = \"squeezebert/squeezebert-uncased\"\n",
        "train_transformer(s140_path, \"Sentiment140\", MODEL_CHECKPOINT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg_OiO2E4p8x"
      },
      "source": [
        "microsoft/deberta-base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ169F1XJHIz"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 3. Training Function (FP16 Disabled)\n",
        "# ==========================================\n",
        "def train_transformer_microsoft(dataset_path, dataset_name, model_checkpoint):\n",
        "    print(f\"\\n{'='*10} Fine-Tuning {model_checkpoint} on {dataset_name} {'='*10}\")\n",
        "\n",
        "    # 1. Load Data\n",
        "    if not os.path.exists(dataset_path):\n",
        "        print(f\"\u274c Error: File not found at {dataset_path}\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(dataset_path).dropna(subset=['clean_text', 'target'])\n",
        "\n",
        "    # Paper Methodology: 50,000 entries per fold\n",
        "    if len(df) > 50000:\n",
        "        print(f\"\u2139\ufe0f Sampling 50,000 rows (Paper Methodology Constraint)...\")\n",
        "        df = df.sample(n=50000, random_state=42)\n",
        "\n",
        "    df['label'] = df['target'].astype(int)\n",
        "    df['clean_text'] = df['clean_text'].astype(str)\n",
        "    df = df[['clean_text', 'label']]\n",
        "\n",
        "    # [cite_start]2. Split Data (80% Train, 20% Eval) [cite: 290]\n",
        "    train_df, eval_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
        "\n",
        "    train_dataset = Dataset.from_pandas(train_df)\n",
        "    eval_dataset = Dataset.from_pandas(eval_df)\n",
        "\n",
        "    # 3. Tokenization\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        return tokenizer(examples[\"clean_text\"], truncation=True, padding=True, max_length=128)\n",
        "\n",
        "    print(\"\ud83d\udd39 Tokenizing data...\")\n",
        "    tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
        "    tokenized_eval = eval_dataset.map(preprocess_function, batched=True)\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    # 4. Initialize Model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n",
        "\n",
        "    # 5. Define Metrics\n",
        "    accuracy_metric = evaluate.load(\"accuracy\")\n",
        "    precision_metric = evaluate.load(\"precision\")\n",
        "    recall_metric = evaluate.load(\"recall\")\n",
        "    f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "        acc = accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "        prec = precision_metric.compute(predictions=predictions, references=labels)\n",
        "        rec = recall_metric.compute(predictions=predictions, references=labels)\n",
        "        f1 = f1_metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "        return {\n",
        "            \"accuracy\": acc[\"accuracy\"],\n",
        "            \"precision\": prec[\"precision\"],\n",
        "            \"recall\": rec[\"recall\"],\n",
        "            \"f1\": f1[\"f1\"],\n",
        "        }\n",
        "\n",
        "    # 6. Training Arguments\n",
        "    # \u26a0\ufe0f KEY CHANGE: fp16=False to prevent DeBERTa overflow error\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results_{dataset_name}_{model_checkpoint.replace('/', '-')}\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=8,   # Reduced batch size to 8 to compensate for higher memory usage of FP32\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=2,\n",
        "        weight_decay=0.01,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        push_to_hub=False,\n",
        "        fp16=False,                     # <--- DISABLED FP16 HERE\n",
        "    )\n",
        "\n",
        "    # 7. Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train,\n",
        "        eval_dataset=tokenized_eval,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # 8. Train & Evaluate\n",
        "    print(\"\ud83d\udd39 Starting Training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"\ud83d\udd39 Final Evaluation:\")\n",
        "    metrics = trainer.evaluate()\n",
        "    print(metrics)\n",
        "\n",
        "    # Cleanup to save RAM\n",
        "    del model\n",
        "    del trainer\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614,
          "referenced_widgets": [
            "2a5c9ffe4c724933852e4dd6ec8c04bf",
            "5344d246baf14ffdae20b7fa520634e9",
            "89ddce4b0f2448fd90b52a5a0e35a96d",
            "27f06f1071934a16bfe9754e2028fbff",
            "9b1bd4257fb34d169f35dbb36f6bc2cc",
            "b9939c2e935c476bb3874fb9e851c1a5",
            "ca9a165d6cf7479bbf4d76e1b75534bd",
            "2455c22d17d245b79191cb2548bcb658",
            "2f240ed0016f49ada9a4cf48b1ec5b65",
            "13ff199ed5b745068140890168e9db1b",
            "8eb042bdc3844d22aafa6b0dac3db6ea",
            "c89454a98c1c4d6d9525103dbe28883e",
            "53d1e4e770c940ea8ab98601b41b920c",
            "963e94fd2fe84db8930e760a36d845cd",
            "4d20b078384e410ea8ce458c1d7bcd2b",
            "3949c5fba471441b9febeaf51ed61f83",
            "e415993177584cf382607851bea2b8f2",
            "fc59536f0e6d4345ab3e0de04f7a4f10",
            "7c751c82e5c049fdae39b04998cb4b13",
            "13c77f0fd92c48e5b28bd25bd6338109",
            "6fe7a65a1901483bb4fe3f12ddac061b",
            "cdf378b6bdb14cf588942824114e1507"
          ]
        },
        "id": "586W50d44qvJ",
        "outputId": "e7223844-acc4-4a04-a212-ab8162ba2c40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========== Fine-Tuning microsoft/deberta-base on Suicide-Watch ==========\n",
            "\u2139\ufe0f Sampling 50,000 rows (Paper Methodology Constraint)...\n",
            "\ud83d\udd39 Tokenizing data...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a5c9ffe4c724933852e4dd6ec8c04bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c89454a98c1c4d6d9525103dbe28883e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-1249932582.py:82: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd39 Starting Training...\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "value cannot be converted to type at::Half without overflow",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1050195832.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# ==========================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Run DistilBERT on Suicide-Watch (Simple Patterns)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_transformer_microsoft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msw_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Suicide-Watch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_CHECKPOINT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1249932582.py\u001b[0m in \u001b[0;36mtrain_transformer_microsoft\u001b[0;34m(dataset_path, dataset_name, model_checkpoint)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# 8. Train & Evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\ud83d\udd39 Starting Training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\ud83d\udd39 Final Evaluation:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4019\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4020\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4022\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4108\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4109\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4110\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4111\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4112\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     decorate_autocast.__script_unsupported = (  # type: ignore[attr-defined]\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m         outputs = self.deberta(\n\u001b[0m\u001b[1;32m   1000\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    708\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0mrel_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_rel_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m             hidden_states, att_m = layer_module(\n\u001b[0m\u001b[1;32m    584\u001b[0m                 \u001b[0mnext_kv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n\u001b[0;32m--> 511\u001b[0;31m         attention_output, att_matrix = self.attention(\n\u001b[0m\u001b[1;32m    512\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n\u001b[0;32m--> 446\u001b[0;31m         self_output, att_matrix = self.self(\n\u001b[0m\u001b[1;32m    447\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0;31m# bsz x height x length x dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: value cannot be converted to type at::Half without overflow"
          ]
        }
      ],
      "source": [
        "\n",
        "MODEL_CHECKPOINT = \"microsoft/deberta-base\"\n",
        "\n",
        "# ==========================================\n",
        "# 4. Execution\n",
        "# ==========================================\n",
        "# Run DistilBERT on Suicide-Watch (Simple Patterns)\n",
        "train_transformer_microsoft(sw_path, \"Suicide-Watch\", MODEL_CHECKPOINT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579,
          "referenced_widgets": [
            "9530baa28c0b4b078e07d2b6365547bf",
            "064d037705394a09bacffe5301d9e37c",
            "78ed3bfd2bf3416499cc5d38870fc57a",
            "ca76812da93d4a43a39e502fee8e59ff",
            "9963112dd73148ee90b377d0c60a48f4",
            "60b9cafbe7fd4da688ff7fd253efcb69",
            "bec5aff54c0d4edaa8d2573a40382d5d",
            "34256926350049a4a4e0fd44cc3134ef",
            "4526a1e07b984fb4b942c07bb0e8b775",
            "f7562d33c4254ec496079a895b85ac21",
            "e5f75643ead140fdac9910c414f62bea",
            "77a075999cfb4aedbbd5710dbabca7e0",
            "6b7755271fa3423daf009c1ebb661108",
            "cc1492dc418040188966ef66c481acfb",
            "6ee737f23f0c44ce9f68b39cba6c7ad5",
            "d5155544b6fb44759fc3c6172d12f4b1",
            "21f29bdb0553453fb3a5a85dbdd115af",
            "96c7ccdd915849d4baaeeaab351fc0be",
            "78093a9ae3874f119bca3164bc492bca",
            "75f5c8e0101c4e5fbddd5c54d1c2e2b1",
            "a4e24181977a4dd499588df125675935",
            "f7bbe632f80942eb86e4a3d4d5c00647"
          ]
        },
        "id": "syJMXALE7RtJ",
        "outputId": "caa7be8e-a7fd-4c2b-b705-0148f80dc3c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========== Fine-Tuning microsoft/deberta-base on Sentiment140 ==========\n",
            "\u2139\ufe0f Sampling 50,000 rows (Paper Methodology Constraint)...\n",
            "\ud83d\udd39 Tokenizing data...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9530baa28c0b4b078e07d2b6365547bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77a075999cfb4aedbbd5710dbabca7e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3829038943.py:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd39 Starting Training...\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "value cannot be converted to type at::Half without overflow",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-367259769.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms140_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Sentiment140\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_CHECKPOINT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3829038943.py\u001b[0m in \u001b[0;36mtrain_transformer\u001b[0;34m(dataset_path, dataset_name, model_checkpoint)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# 8. Train & Evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\ud83d\udd39 Starting Training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\ud83d\udd39 Final Evaluation:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4019\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4020\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4022\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4108\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4109\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4110\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4111\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4112\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     decorate_autocast.__script_unsupported = (  # type: ignore[attr-defined]\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m         outputs = self.deberta(\n\u001b[0m\u001b[1;32m   1000\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    708\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0mrel_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_rel_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m             hidden_states, att_m = layer_module(\n\u001b[0m\u001b[1;32m    584\u001b[0m                 \u001b[0mnext_kv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n\u001b[0;32m--> 511\u001b[0;31m         attention_output, att_matrix = self.attention(\n\u001b[0m\u001b[1;32m    512\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n\u001b[0;32m--> 446\u001b[0;31m         self_output, att_matrix = self.self(\n\u001b[0m\u001b[1;32m    447\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0;31m# bsz x height x length x dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: value cannot be converted to type at::Half without overflow"
          ]
        }
      ],
      "source": [
        "train_transformer_microsoft(s140_path, \"Sentiment140\", MODEL_CHECKPOINT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRkIUT3zGZWN"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzUFg2RMGZbR"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H29SkHa6GZeZ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU3svGYmGZ13",
        "outputId": "9f7be240-8390-400e-e501-cc2e95f544b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 GPU Memory Cleared\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# Delete potential leftovers\n",
        "try:\n",
        "    del model\n",
        "    del trainer\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"\u2705 GPU Memory Cleared\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g4XtMa-Gc9V",
        "outputId": "72b4c6aa-6e13-42d9-93ad-84e9fa3f9898"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "Some parameter dtypes (sample): [torch.float32]\n",
            "logits dtype: torch.float32\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model_checkpoint = \"microsoft/deberta-base\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "# Print first 10 parameter dtypes\n",
        "dtypes = list({p.dtype for p in list(model.parameters())[:10]})\n",
        "print(\"Some parameter dtypes (sample):\", dtypes)\n",
        "\n",
        "# Do a tiny dummy forward (CPU) to check dtype of computed tensors\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    ids = torch.tensor([[0,1,2,3,4,5]])\n",
        "    mask = torch.ones_like(ids)\n",
        "    out = model(input_ids=ids, attention_mask=mask)\n",
        "    # find dtype of an internal output if present\n",
        "    if hasattr(out, \"logits\"):\n",
        "        print(\"logits dtype:\", out.logits.dtype)\n",
        "    else:\n",
        "        print(\"output type:\", type(out))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627,
          "referenced_widgets": [
            "51de2fb7c8db40dcaccb3c6d63609957",
            "e2151b16c31a417aac16b7a3f89b18ff",
            "5391f71236cf49e3afd96f05c6c893f1",
            "d2dd4ef5a00c49eeb9f8bf0568fbd777",
            "708d81bb048843ec878824125c60f595",
            "ac9f84c08a174e0d804e67d0d542a89c",
            "293d4f511aac4b3683c53fb39ee85883",
            "8c6f0cbe7075464c91020c7bb6410f7f",
            "745cb4e0ab6844d992b253a30299185e",
            "d697b1aa7bc342338e86de8c7e6602a0",
            "06407c2257124bdb829c67293106ba8c",
            "9d562f5c80dd4b38acf4c1c67b8f7a89",
            "67724d474f2b44ed86df1477584bee12",
            "d83943b4483445d1a3118c715ec32e84",
            "532f626dfd5d4a83ae172320f7dddb87",
            "a024ebcd53c746d9b5935b0249425d95",
            "d711f3f3ce2b436fa818b4565fd4cfde",
            "1edd66cb876143679c9214b97329f531",
            "0a8c00bfc3eb43f096426e817747d62b",
            "c5c26ea053ae4649a76bf6b37ade3eba",
            "489ee6fa38e04166849850b500c7e76a",
            "df9b4168527346228dae81961ae086f4",
            "65658cf9c9f54715aca624990844637a",
            "ea6eadb30e7a4bacadb64200375024e3",
            "b0bafce2ba3a417b8c1860cb2817f940",
            "4453f5f11c354076bf14618adcd2dc51",
            "0cb3323b58c547a4bcf78109199ad8f0",
            "316158af241e43b6b12c510364af28c8",
            "954456313fea40dab1c041e7bb201c86",
            "8e73ed287ae6482aaf144aae8480690b",
            "751b142b0cdc45328ec4e4c4241badae",
            "9f93fd125936418ca55e92d02849c0bd",
            "bbab4be393114275baff30424570becb",
            "3f61bfa48cf045d5b6da016b446b69d9",
            "df28fcbc66594f03988203bb722d11a8",
            "515db8f58a4945d881cf6275f606188d",
            "e893d8dbc3394f859f792a3f48e7ebdb",
            "7c78bfcd77f243cbabdd62247a7e9777",
            "80d830c725c84feb8007c98d2f9a1c49",
            "c113733722cd41d58d38430ffd7ec221",
            "da448cbd87e246e583339a071c1ac6d8",
            "c8ab6ff666aa4fe7af4d8df126f9ca3f",
            "c1f261ac56ba44d984b7635863df4fcd",
            "1b634c32ac224dac808d580ba55978b9",
            "bf326744ddc849518fdff1c4ca4a0674",
            "317cd77052964ae597736c717245e445",
            "c8e66acc52494ceca36f6b7c6e0ac822",
            "a8e38eac627a44ebba924ffdf06f6daf",
            "eafab58c327a4115b7845a39ce8ab518",
            "1dfd8238b38f4c7ea0ba5923052963c9",
            "8e354cca7c6444f4941e160df0d38cd8",
            "a7933a461d5044579c4d9a66e9bbaf9b",
            "b150e108e65b404a87e47eb9e5ecf622",
            "4eadaf6458e246649f3ff5898e8ecb84",
            "0ff8eb0a417d40698366f6e6f74ef42b",
            "cb8fcfe794374b1eb646ac6b4a65cdca",
            "2ae4e44ad99e43c4b2d212313f8ada82",
            "9cb46f395ff54d0592b3335f9534982d",
            "c4b21b2d4a7b4a7ab46be8aa10947657",
            "992fb3e67cc746e284385ee4859d84a5",
            "1b0425989fe241edb4be027b9814fb0c",
            "43e5df7a07c74a209cf0156b72f13efb",
            "40a2ff4201ca4948a890d0aaf870836e",
            "8940663860f14e77ba8e6d0357b0bc64",
            "ed5f5d75d99443d4ac0cb38896d752e3",
            "f0fd9355f6d54d83b48a8133fd007fee",
            "9b3a74739cb34660b1c5673ae2443f95",
            "07d80087b5fe42879731d2f9cbe71cc9",
            "a9e3460b0ebc4788a157f51367920753",
            "940b853fd76b4f7a82d05c2a85a87e7d",
            "c594344b297c4fcca864f4115c2b04e0",
            "440b2af213804ba6bcbcebad8c118698",
            "6b2cf1f772554ea6b81a2d313dc6af93",
            "9ee64b3323af4651bd6cc784c47305a4",
            "13f36ae1d2904a5a95dc9407cded43bb",
            "d3bd23f7d98f475c9311ed2ac80d97e9",
            "0b0eba4731df4035b63c5fd29e1a6530"
          ]
        },
        "id": "JdLtudpGI9LB",
        "outputId": "329c0caf-786e-43b2-95f8-d4b40a6210a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Data Loaded: 50000 rows\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51de2fb7c8db40dcaccb3c6d63609957",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d562f5c80dd4b38acf4c1c67b8f7a89",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "65658cf9c9f54715aca624990844637a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd39 Tokenizing...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f61bfa48cf045d5b6da016b446b69d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf326744ddc849518fdff1c4ca4a0674",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb8fcfe794374b1eb646ac6b4a65cdca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b3a74739cb34660b1c5673ae2443f95",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/371M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-106645038.py:103: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd39 Starting Training (DeBERTa V3)...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10000/10000 28:33, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u2705 Final Evaluation:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1250/1250 00:40]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 9.536742923144104e-11, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_runtime': 40.2657, 'eval_samples_per_second': 248.35, 'eval_steps_per_second': 31.044, 'epoch': 2.0}\n"
          ]
        }
      ],
      "source": [
        "# @title  Training DeBERTa V3 (Stable)\n",
        "# 1. Install Dependencies (V3 requires sentencepiece)\n",
        "!pip install -q transformers accelerate evaluate datasets sentencepiece\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import (\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        ")\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import evaluate\n",
        "\n",
        "# 2. Setup Paths & Config\n",
        "# Update this path if needed\n",
        "base_path = '/content/drive/MyDrive/Depression_Paper_DL'\n",
        "csv_path = os.path.join(base_path, 'suicidewatch_cleaned.csv')\n",
        "\n",
        "# SWITCHING TO V3 FOR STABILITY\n",
        "MODEL_CHECKPOINT = \"microsoft/deberta-v3-base\"\n",
        "\n",
        "# 3. Load & Clean Data\n",
        "if not os.path.exists(csv_path):\n",
        "    print(f\"\u26a0\ufe0f File not found at {csv_path}. Checking local directory...\")\n",
        "    if os.path.exists(\"suicidewatch_cleaned.csv\"):\n",
        "        csv_path = \"suicidewatch_cleaned.csv\"\n",
        "    else:\n",
        "        # Fallback to creating dummy data if you are just testing the code structure\n",
        "        print(\"\u274c Dataset not found. Please upload 'suicidewatch_full_cleaned.csv'\")\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Strict Cleaning\n",
        "    df['target'] = pd.to_numeric(df['target'], errors='coerce')\n",
        "    df = df.dropna(subset=['clean_text', 'target'])\n",
        "    df = df[df['target'].isin([0, 1])]\n",
        "\n",
        "    # Sampling 50k\n",
        "    if len(df) > 50000:\n",
        "        df = df.sample(n=50000, random_state=42)\n",
        "\n",
        "    df['label'] = df['target'].astype(int)\n",
        "    df['clean_text'] = df['clean_text'].astype(str)\n",
        "\n",
        "    print(f\"\u2705 Data Loaded: {len(df)} rows\")\n",
        "\n",
        "    # 4. Split\n",
        "    train_df, eval_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
        "    train_dataset = Dataset.from_pandas(train_df)\n",
        "    eval_dataset = Dataset.from_pandas(eval_df)\n",
        "\n",
        "    # 5. Tokenizer (V3)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        return tokenizer(examples[\"clean_text\"], truncation=True, padding=True, max_length=128)\n",
        "\n",
        "    print(\"\ud83d\udd39 Tokenizing...\")\n",
        "    tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
        "    tokenized_eval = eval_dataset.map(preprocess_function, batched=True)\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    # 6. Model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=2)\n",
        "\n",
        "    # 7. Metrics\n",
        "    accuracy = evaluate.load(\"accuracy\")\n",
        "    f1 = evaluate.load(\"f1\")\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "        return {\n",
        "            \"accuracy\": accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
        "            \"f1\": f1.compute(predictions=predictions, references=labels)[\"f1\"],\n",
        "        }\n",
        "\n",
        "    # 8. Training Arguments\n",
        "    # V3 is usually stable with FP16, but we keep False just to be 100% safe given your errors.\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results_deberta_v3\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=2,\n",
        "        weight_decay=0.01,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        push_to_hub=False,\n",
        "        fp16=False, # Keeping FP32 for maximum safety\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train,\n",
        "        eval_dataset=tokenized_eval,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    print(\"\ud83d\udd39 Starting Training (DeBERTa V3)...\")\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"\\n\u2705 Final Evaluation:\")\n",
        "    metrics = trainer.evaluate()\n",
        "    print(metrics)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n\u274c An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467,
          "referenced_widgets": [
            "01f2f00608244639847f7e84f37b0816",
            "8fd999582d214611a8f2bf493dc8e830",
            "682edb3c56694005bec779b5005524d8",
            "de36cb82e13b488abe230bade7891cb2",
            "60cb525cf3f046a39ef73410826b2c2c",
            "4851ea630c4c493abb04289cc1f90841",
            "41daaca8fec04c01af41853696d4d1b8",
            "4bd03e3f14654fa8bdbc9ffd81186dd1",
            "88c98cc3045f430c8c50d9c2a571771a",
            "b2b9daf7cdf14c4ea3e7ee59b82fe845",
            "43e3659a502647fcb159367abc2408bc",
            "b95839545e2b439f93dabba4f9c565fd",
            "64299dd0ab304a2eaa179c99d4db6368",
            "3cad6806c578491ea58658e85f064867",
            "442006ed579e4ff6b53bc0c9d78f7412",
            "bb3412dc88d143d590c2e94c0e072d17",
            "5cd73e90f3ab41748f842f86e49ef802",
            "ac7bf227361d4f14a04c22063da6bd80",
            "a40f5c2c394d48a3b9c4ba7b4c3de555",
            "6ffb1ce9b7a74124992929c092b96f18",
            "83ef8591edf34cccb286b57f5d58491c",
            "514634db90c74fe19bbe0c0df7112736"
          ]
        },
        "id": "TXkVUH8uJBvJ",
        "outputId": "bd52ba23-a6ca-4fbf-b394-02c8fe2c6466"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Data Loaded: 50000 rows\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd39 Tokenizing...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01f2f00608244639847f7e84f37b0816",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b95839545e2b439f93dabba4f9c565fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-3952099572.py:103: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd39 Starting Training (DeBERTa V3)...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10000/10000 24:19, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.457000</td>\n",
              "      <td>0.485880</td>\n",
              "      <td>0.787800</td>\n",
              "      <td>0.768138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.379100</td>\n",
              "      <td>0.495064</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.799920</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u2705 Final Evaluation:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1250/1250 00:36]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.4858803153038025, 'eval_accuracy': 0.7878, 'eval_f1': 0.7681381118881119, 'eval_runtime': 36.3848, 'eval_samples_per_second': 274.84, 'eval_steps_per_second': 34.355, 'epoch': 2.0}\n"
          ]
        }
      ],
      "source": [
        "# @title P Training DeBERTa V3 (Stable)\n",
        "# 1. Install Dependencies (V3 requires sentencepiece)\n",
        "!pip install -q transformers accelerate evaluate datasets sentencepiece\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import (\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        ")\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import evaluate\n",
        "\n",
        "# 2. Setup Paths & Config\n",
        "# Update this path if needed\n",
        "base_path = '/content/drive/MyDrive/Depression_Paper_DL'\n",
        "csv_path = os.path.join(base_path, 'sentiment140_cleaned.csv')\n",
        "\n",
        "# SWITCHING TO V3 FOR STABILITY\n",
        "MODEL_CHECKPOINT = \"microsoft/deberta-v3-base\"\n",
        "\n",
        "# 3. Load & Clean Data\n",
        "if not os.path.exists(csv_path):\n",
        "    print(f\"\u26a0\ufe0f File not found at {csv_path}. Checking local directory...\")\n",
        "    if os.path.exists(\"suicidewatch_cleaned.csv\"):\n",
        "        csv_path = \"suicidewatch_cleaned.csv\"\n",
        "    else:\n",
        "        # Fallback to creating dummy data if you are just testing the code structure\n",
        "        print(\"\u274c Dataset not found. Please upload 'suicidewatch_full_cleaned.csv'\")\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Strict Cleaning\n",
        "    df['target'] = pd.to_numeric(df['target'], errors='coerce')\n",
        "    df = df.dropna(subset=['clean_text', 'target'])\n",
        "    df = df[df['target'].isin([0, 1])]\n",
        "\n",
        "    # Sampling 50k\n",
        "    if len(df) > 50000:\n",
        "        df = df.sample(n=50000, random_state=42)\n",
        "\n",
        "    df['label'] = df['target'].astype(int)\n",
        "    df['clean_text'] = df['clean_text'].astype(str)\n",
        "\n",
        "    print(f\"\u2705 Data Loaded: {len(df)} rows\")\n",
        "\n",
        "    # 4. Split\n",
        "    train_df, eval_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
        "    train_dataset = Dataset.from_pandas(train_df)\n",
        "    eval_dataset = Dataset.from_pandas(eval_df)\n",
        "\n",
        "    # 5. Tokenizer (V3)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        return tokenizer(examples[\"clean_text\"], truncation=True, padding=True, max_length=128)\n",
        "\n",
        "    print(\"\ud83d\udd39 Tokenizing...\")\n",
        "    tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
        "    tokenized_eval = eval_dataset.map(preprocess_function, batched=True)\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    # 6. Model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=2)\n",
        "\n",
        "    # 7. Metrics\n",
        "    accuracy = evaluate.load(\"accuracy\")\n",
        "    f1 = evaluate.load(\"f1\")\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "        return {\n",
        "            \"accuracy\": accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
        "            \"f1\": f1.compute(predictions=predictions, references=labels)[\"f1\"],\n",
        "        }\n",
        "\n",
        "    # 8. Training Arguments\n",
        "    # V3 is usually stable with FP16, but we keep False just to be 100% safe given your errors.\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results_deberta_v3\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=2,\n",
        "        weight_decay=0.01,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        push_to_hub=False,\n",
        "        fp16=False, # Keeping FP32 for maximum safety\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train,\n",
        "        eval_dataset=tokenized_eval,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    print(\"\ud83d\udd39 Starting Training (DeBERTa V3)...\")\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"\\n\u2705 Final Evaluation:\")\n",
        "    metrics = trainer.evaluate()\n",
        "    print(metrics)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n\u274c An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t9BnwH7V5Ye"
      },
      "source": [
        "#Phase6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuvNXtezV8XF",
        "outputId": "9ff675a4-7950-46a4-f79b-3b133efebf63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Testing Custom Inputs (Traditional ML) ---\n",
            "\u26a0\ufe0f Pipeline not found in memory. Run Phase 3 first.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# @title Inference with Logistic Regression (CPU/GPU)\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# 1. Setup Preprocessing\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    tokens = text.split()\n",
        "    filtered_tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
        "    return \" \".join(filtered_tokens)\n",
        "\n",
        "# 2. Prediction Function\n",
        "def predict_depression_ml(text, vectorizer, model):\n",
        "    # Clean\n",
        "    clean_text = preprocess_text(text)\n",
        "    # Vectorize\n",
        "    vectorized_text = vectorizer.transform([clean_text])\n",
        "    # Predict\n",
        "    try:\n",
        "        prediction = model.predict(vectorized_text)[0]\n",
        "        label = \"Depressed\" if prediction == 1 else \"Non-Depressed\"\n",
        "        return label\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "# 3. Usage Example\n",
        "\n",
        "print(\"\\n--- Testing Custom Inputs (Traditional ML) ---\")\n",
        "custom_tweets = [\n",
        "    \"I feel absolutely hopeless and I don't see a way out of this darkness.\",\n",
        "    \"Had a great day at the park with friends! heavy sunshine.\",\n",
        "    \"I'm so tired of trying, nothing ever gets better.\"\n",
        "]\n",
        "\n",
        "if 'pipeline' in locals():\n",
        "    # Extract parts from the pipeline for clarity\n",
        "    vect = pipeline.named_steps['tfidfvectorizer']\n",
        "    clf = pipeline.named_steps['logisticregression']\n",
        "\n",
        "    for tweet in custom_tweets:\n",
        "        result = predict_depression_ml(tweet, vect, clf)\n",
        "        print(f\"Input: '{tweet}'\\nPrediction: {result}\\n\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f Pipeline not found in memory. Run Phase 3 first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeR2jADcWKdd",
        "outputId": "ba3285f3-689f-489f-dccd-c8a1adcfa1ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from: ./results_Suicide-Watch_distilbert-base-uncased/checkpoint-5000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Testing Custom Inputs (Transformer) ---\n",
            "\u26a0\ufe0f Error: CUDA error: device-side assert triggered\n",
            "Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title Inference with Transformer (RoBERTa/DistilBERT)\n",
        "from transformers import pipeline as hf_pipeline\n",
        "\n",
        "import glob\n",
        "import os\n",
        "\n",
        "result_dir = \"./results_Suicide-Watch_distilbert-base-uncased\"\n",
        "\n",
        "try:\n",
        "    # Find the latest checkpoint folder\n",
        "    checkpoints = glob.glob(f\"{result_dir}/checkpoint-*\")\n",
        "    latest_checkpoint = max(checkpoints, key=os.path.getctime)\n",
        "    print(f\"Loading model from: {latest_checkpoint}\")\n",
        "\n",
        "    # 2. Create Inference Pipeline' tool\n",
        "    classifier = hf_pipeline(\"text-classification\", model=latest_checkpoint, tokenizer=\"roberta-base\")\n",
        "\n",
        "    # 3. Prediction Function\n",
        "    def predict_depression_transformer(text):\n",
        "        clean_text = preprocess_text(text)\n",
        "        result = classifier(clean_text)\n",
        "\n",
        "        label_map = {'LABEL_1': 'Depressed', 'LABEL_0': 'Non-Depressed'}\n",
        "        label_str = label_map.get(result[0]['label'], \"Unknown\")\n",
        "        score = result[0]['score']\n",
        "\n",
        "        return label_str, score\n",
        "\n",
        "    print(\"\\n--- Testing Custom Inputs (Transformer) ---\")\n",
        "    custom_tweets = [\n",
        "        \"I feel absolutely hopeless and I don't see a way out of this darkness.\",\n",
        "        \"Had a great day at the park with friends! heavy sunshine.\",\n",
        "        \"I'm so tired of trying, nothing ever gets better.\"\n",
        "    ]\n",
        "\n",
        "    for tweet in custom_tweets:\n",
        "        label, conf = predict_depression_transformer(tweet)\n",
        "        print(f\"Input: '{tweet}'\\nPrediction: {label} (Confidence: {conf:.4f})\\n\")\n",
        "\n",
        "except ValueError:\n",
        "    print(f\"\u26a0\ufe0f Checkpoint not found in {result_dir}. Did Phase 4 finish successfully?\")\n",
        "except Exception as e:\n",
        "    print(f\"\u26a0\ufe0f Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MODIFICATION (XG BOOST, XLM-RoBERTa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import xgboost as xgb\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Function for XLM-RoBERTa model implementation\n",
        "def train_xlm_roberta_model(X_train, y_train, X_val, y_val, num_epochs=3, batch_size=16):\n",
        "    \"\"\"\n",
        "    Train XLM-RoBERTa model for text classification\n",
        "    \"\"\"\n",
        "    print(\"Initializing XLM-RoBERTa model...\")\n",
        "    \n",
        "    # Load pre-trained XLM-RoBERTa model and tokenizer\n",
        "    model_name = 'xlm-roberta-base'\n",
        "    tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
        "    model = XLMRobertaForSequenceClassification.from_pretrained(\n",
        "        model_name, \n",
        "        num_labels=len(np.unique(y_train))\n",
        "    )\n",
        "    \n",
        "    # Move model to GPU if available\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    \n",
        "    # Tokenize the training and validation data\n",
        "    print(\"Tokenizing data...\")\n",
        "    \n",
        "    train_encodings = tokenizer(\n",
        "        X_train.tolist(),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=128,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    \n",
        "    val_encodings = tokenizer(\n",
        "        X_val.tolist(),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=128,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    \n",
        "    # Create PyTorch datasets\n",
        "    train_dataset = TensorDataset(\n",
        "        train_encodings['input_ids'],\n",
        "        train_encodings['attention_mask'],\n",
        "        torch.tensor(y_train)\n",
        "    )\n",
        "    \n",
        "    val_dataset = TensorDataset(\n",
        "        val_encodings['input_ids'],\n",
        "        val_encodings['attention_mask'],\n",
        "        torch.tensor(y_val)\n",
        "    )\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "    \n",
        "    # Set up optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "    \n",
        "    # Training loop\n",
        "    print(\"Training XLM-RoBERTa model...\")\n",
        "    model.train()\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "            \n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "    \n",
        "    # Validation\n",
        "    print(\"Evaluating XLM-RoBERTa model...\")\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "            \n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    print(f\"\\nXLM-RoBERTa Validation Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(true_labels, predictions))\n",
        "    \n",
        "    return model, tokenizer, accuracy\n",
        "\n",
        "# Function for XGBoost model implementation\n",
        "def train_xgboost_model(X_train, y_train, X_val, y_val, use_bert_embeddings=True, bert_model=None, tokenizer=None):\n",
        "    \"\"\"\n",
        "    Train XGBoost model for text classification\n",
        "    Option to use BERT embeddings or TF-IDF features\n",
        "    \"\"\"\n",
        "    print(\"Training XGBoost model...\")\n",
        "    \n",
        "    # Convert text to features\n",
        "    if use_bert_embeddings and bert_model is not None and tokenizer is not None:\n",
        "        print(\"Extracting BERT embeddings for XGBoost...\")\n",
        "        X_train_features = extract_bert_embeddings(bert_model, tokenizer, X_train)\n",
        "        X_val_features = extract_bert_embeddings(bert_model, tokenizer, X_val)\n",
        "    else:\n",
        "        # Use TF-IDF as fallback\n",
        "        print(\"Using TF-IDF features for XGBoost...\")\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        \n",
        "        vectorizer = TfidfVectorizer(max_features=5000)\n",
        "        X_train_features = vectorizer.fit_transform(X_train).toarray()\n",
        "        X_val_features = vectorizer.transform(X_val).toarray()\n",
        "    \n",
        "    # Train XGBoost model\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=6,\n",
        "        random_state=42,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='mlogloss'\n",
        "    )\n",
        "    \n",
        "    xgb_model.fit(X_train_features, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = xgb_model.predict(X_val_features)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    print(f\"\\nXGBoost Validation Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_val, y_pred))\n",
        "    \n",
        "    return xgb_model, accuracy\n",
        "\n",
        "def extract_bert_embeddings(model, tokenizer, texts, batch_size=16):\n",
        "    \"\"\"\n",
        "    Extract BERT embeddings from text\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    all_embeddings = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        \n",
        "        # Tokenize\n",
        "        encodings = tokenizer(\n",
        "            batch_texts.tolist(),\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=128,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        # Move to device\n",
        "        input_ids = encodings['input_ids'].to(device)\n",
        "        attention_mask = encodings['attention_mask'].to(device)\n",
        "        \n",
        "        # Get embeddings\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                output_hidden_states=True\n",
        "            )\n",
        "            \n",
        "            # Use last hidden state [CLS] token\n",
        "            last_hidden_state = outputs.hidden_states[-1]\n",
        "            cls_embeddings = last_hidden_state[:, 0, :]\n",
        "            all_embeddings.append(cls_embeddings.cpu().numpy())\n",
        "    \n",
        "    return np.vstack(all_embeddings)\n",
        "\n",
        "# Main training function combining both models\n",
        "def train_hybrid_model(X_train, y_train, X_val, y_val):\n",
        "    \"\"\"\n",
        "    Train XLM-RoBERTa and XGBoost models\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"Training XLM-RoBERTa Model\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Train XLM-RoBERTa\n",
        "    xlm_model, tokenizer, xlm_accuracy = train_xlm_roberta_model(\n",
        "        X_train, y_train, X_val, y_val,\n",
        "        num_epochs=3,\n",
        "        batch_size=16\n",
        "    )\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Training XGBoost with BERT Embeddings\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Train XGBoost using XLM-RoBERTa embeddings\n",
        "    xgb_model, xgb_accuracy = train_xgboost_model(\n",
        "        X_train, y_train, X_val, y_val,\n",
        "        use_bert_embeddings=True,\n",
        "        bert_model=xlm_model,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "    \n",
        "    # Create ensemble predictions (simple voting)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Creating Ensemble Predictions\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Get XLM-RoBERTa predictions\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    xlm_model.eval()\n",
        "    \n",
        "    val_encodings = tokenizer(\n",
        "        X_val.tolist(),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=128,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    \n",
        "    val_dataset = TensorDataset(\n",
        "        val_encodings['input_ids'],\n",
        "        val_encodings['attention_mask'],\n",
        "        torch.tensor(y_val)\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "    \n",
        "    xlm_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, attention_mask, _ = batch\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            \n",
        "            outputs = xlm_model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "            \n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            xlm_predictions.extend(preds.cpu().numpy())\n",
        "    \n",
        "    # Get XGBoost predictions\n",
        "    val_features = extract_bert_embeddings(xlm_model, tokenizer, X_val)\n",
        "    xgb_predictions = xgb_model.predict(val_features)\n",
        "    \n",
        "    # Ensemble voting\n",
        "    ensemble_predictions = []\n",
        "    for xlm_pred, xgb_pred in zip(xlm_predictions, xgb_predictions):\n",
        "        # Simple voting (you can modify this logic)\n",
        "        if xlm_pred == xgb_pred:\n",
        "            ensemble_predictions.append(xlm_pred)\n",
        "        else:\n",
        "            # In case of disagreement, use XLM-RoBERTa (usually more accurate for text)\n",
        "            ensemble_predictions.append(xlm_pred)\n",
        "    \n",
        "    # Calculate ensemble accuracy\n",
        "    ensemble_accuracy = accuracy_score(y_val, ensemble_predictions)\n",
        "    print(f\"\\nEnsemble Model Accuracy: {ensemble_accuracy:.4f}\")\n",
        "    print(\"\\nEnsemble Classification Report:\")\n",
        "    print(classification_report(y_val, ensemble_predictions))\n",
        "    \n",
        "    return {\n",
        "        'xlm_roberta': {'model': xlm_model, 'tokenizer': tokenizer, 'accuracy': xlm_accuracy},\n",
        "        'xgboost': {'model': xgb_model, 'accuracy': xgb_accuracy},\n",
        "        'ensemble_accuracy': ensemble_accuracy\n",
        "    }\n",
        "\n",
        "# Example usage in your notebook\n",
        "\"\"\"\n",
        "# Assuming you have your data loaded as:\n",
        "# X_train, X_val, y_train, y_val\n",
        "\n",
        "# Train the hybrid model\n",
        "results = train_hybrid_model(X_train, y_train, X_val, y_val)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"FINAL RESULTS\")\n",
        "print('='*60)\n",
        "print(f\"XLM-RoBERTa Accuracy: {results['xlm_roberta']['accuracy']:.4f}\")\n",
        "print(f\"XGBoost Accuracy: {results['xgboost']['accuracy']:.4f}\")\n",
        "print(f\"Ensemble Accuracy: {results['ensemble_accuracy']:.4f}\")\n",
        "print('='*60)\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}